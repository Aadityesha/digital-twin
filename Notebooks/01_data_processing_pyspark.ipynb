{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdf171c",
   "metadata": {},
   "source": [
    "1. Spark Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7f6e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session started\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Comcast Digital Twin - Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e4107",
   "metadata": {},
   "source": [
    " 2. Load the CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f608502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- node_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- cpu_usage: double (nullable = true)\n",
      " |-- latency: double (nullable = true)\n",
      " |-- throughput: double (nullable = true)\n",
      " |-- packet_loss: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- error_rate: double (nullable = true)\n",
      " |-- failure_label: integer (nullable = true)\n",
      "\n",
      "+---------+--------------------+------------------+------------------+------------------+-------------------+-----------------+-------------------+-------------+\n",
      "|  node_id|           timestamp|         cpu_usage|           latency|        throughput|        packet_loss|      temperature|         error_rate|failure_label|\n",
      "+---------+--------------------+------------------+------------------+------------------+-------------------+-----------------+-------------------+-------------+\n",
      "|NODE-1000|2025-04-12 23:59:...| 21.65024273930277| 22.50989903031263| 128.8664631227477| 1.2525283013552362|59.45818753855583| 0.5433606347984982|            0|\n",
      "|NODE-1000|2025-04-13 00:09:...|28.460248694267477|  22.1450268348712| 94.98258009738434| 2.5133706222367214|66.26767150969349| 0.4615166620379736|            0|\n",
      "|NODE-1000|2025-04-13 00:19:...|31.629683971304825|27.125946586721945|  98.5162914546391|0.11731107744023583|65.85668799361488|  1.108900089737807|            0|\n",
      "|NODE-1000|2025-04-13 00:29:...|40.988281266370436|29.862758815131837| 74.19396136576623| 3.0061973382803626|55.73791189408985|0.09029582689601394|            1|\n",
      "|NODE-1000|2025-04-13 00:39:...| 34.99904017572465|23.073473215354984|127.96983637335535|0.23789685125934323|61.66202941772495|0.23801102182469983|            0|\n",
      "+---------+--------------------+------------------+------------------+------------------+-------------------+-----------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"../data/simulated_network_telemetry.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60353c3",
   "metadata": {},
   "source": [
    "3. Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae46017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+-------------+\n",
      "|  node_id|           timestamp|         cpu_usage|           latency|       throughput|        packet_loss|      temperature|        error_rate|failure_label|\n",
      "+---------+--------------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+-------------+\n",
      "|NODE-1000|2025-04-12 23:59:...| 21.65024273930277| 22.50989903031263|128.8664631227477| 1.2525283013552362|59.45818753855583|0.5433606347984982|            0|\n",
      "|NODE-1000|2025-04-13 00:09:...|28.460248694267477|  22.1450268348712|94.98258009738434| 2.5133706222367214|66.26767150969349|0.4615166620379736|            0|\n",
      "|NODE-1000|2025-04-13 00:19:...|31.629683971304825|27.125946586721945| 98.5162914546391|0.11731107744023583|65.85668799361488| 1.108900089737807|            0|\n",
      "+---------+--------------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Drop rows with nulls (if any)\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Cast timestamp to proper type\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "df_clean = df_clean.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "df_clean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ef995",
   "metadata": {},
   "source": [
    "4. Feature Engineering: Add Hour, Day, Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb3a2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+\n",
      "|           timestamp|hour|dayofweek|\n",
      "+--------------------+----+---------+\n",
      "|2025-04-12 23:59:...|  23|        7|\n",
      "|2025-04-13 00:09:...|   0|        1|\n",
      "|2025-04-13 00:19:...|   0|        1|\n",
      "|2025-04-13 00:29:...|   0|        1|\n",
      "|2025-04-13 00:39:...|   0|        1|\n",
      "+--------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek\n",
    "\n",
    "df_feat = df_clean \\\n",
    "    .withColumn(\"hour\", hour(\"timestamp\")) \\\n",
    "    .withColumn(\"dayofweek\", dayofweek(\"timestamp\"))\n",
    "\n",
    "df_feat.select(\"timestamp\", \"hour\", \"dayofweek\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c93b5",
   "metadata": {},
   "source": [
    "5. Rolling Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af69c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+------------------+\n",
      "|  node_id|           timestamp|         cpu_usage|   rolling_avg_cpu|\n",
      "+---------+--------------------+------------------+------------------+\n",
      "|NODE-1000|2025-04-12 23:59:...| 21.65024273930277| 21.65024273930277|\n",
      "|NODE-1000|2025-04-13 00:09:...|28.460248694267477|25.055245716785123|\n",
      "|NODE-1000|2025-04-13 00:19:...|31.629683971304825|27.246725134958357|\n",
      "|NODE-1000|2025-04-13 00:29:...|40.988281266370436|30.682114167811378|\n",
      "|NODE-1000|2025-04-13 00:39:...| 34.99904017572465| 34.01931352691685|\n",
      "+---------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "windowSpec = Window.partitionBy(\"node_id\").orderBy(\"timestamp\").rowsBetween(-3, 0)\n",
    "\n",
    "df_feat = df_feat.withColumn(\"rolling_avg_cpu\", avg(\"cpu_usage\").over(windowSpec))\n",
    "df_feat.select(\"node_id\", \"timestamp\", \"cpu_usage\", \"rolling_avg_cpu\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1f0bc",
   "metadata": {},
   "source": [
    "6. Save Cleaned & Enriched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5c66ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aadit\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved using Pandas instead of Spark.\n"
     ]
    }
   ],
   "source": [
    "df_feat_pd = df_feat.toPandas()  # Convert to Pandas\n",
    "df_feat_pd.to_csv(\"../data/processed_network_data.csv\", index=False)  # Save normally\n",
    "print(\"✅ Saved using Pandas instead of Spark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661210f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
